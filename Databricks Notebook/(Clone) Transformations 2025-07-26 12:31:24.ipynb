{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dc49ff-080b-4cc6-af08-e679d6214e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test CSV file read from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5537c4-4008-4509-a9fd-32153503a461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(header=True, inferSchema=True).load('abfss://converted-table-to-csv-files@blobstorageforcsvfiles.dfs.core.windows.net/dbo_dim_brand_snowflake.csv')\n",
    "\n",
    "\n",
    "df.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ca3881-65e2-4b78-9e21-3101373c05f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read only wanted CSVs using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921493fa-09a7-46b5-bc8b-ef324b2707fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "files = dbutils.fs.ls(\"abfss://converted-table-to-csv-files@blobstorageforcsvfiles.dfs.core.windows.net/\")\n",
    "final_df= None\n",
    "allowed_files = {\n",
    "    \"dbo_dim_customer.csv\",\n",
    "    \"dbo_dim_date.csv\",\n",
    "    \"dbo_dim_product.csv\",\n",
    "    \"dbo_dim_store.csv\",\n",
    "    \"dbo_fact_sales.csv\"\n",
    "}\n",
    "for file in files:\n",
    "    if file.name in allowed_files:\n",
    "        print(f'Reading:{file.name}')\n",
    "        df = spark.read.format('csv').options(header = True, inferSchema = True).load(file.path)\n",
    "        df.printSchema()\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39f2b63-b58c-4bc4-899e-50fb3409d539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read Raw CSVs from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a356a8-5726-4843-acbf-3652647a1d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"abfss://converted-table-to-csv-files@blobstorageforcsvfiles.dfs.core.windows.net/\"\n",
    "\n",
    "def read_csv(file_name):\n",
    "    return spark.read.format('csv').options(header = True, inferSchema = True).load(f\"{file_path}{file_name}.csv\")\n",
    "\n",
    "df_customer = read_csv(\"dbo_dim_customer\")\n",
    "df_product = read_csv(\"dbo_dim_product\")\n",
    "df_store   = read_csv(\"dbo_dim_store\")\n",
    "df_date    = read_csv(\"dbo_dim_date\")\n",
    "df_fact    = read_csv(\"dbo_fact_sales\")\n",
    "df_staging = read_csv(\"dbo_staging_fact_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa651dd5-2859-42c1-828b-d35ed93519e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cleaned data and validated by dropping null values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4be7cf-5ce1-43de-aa79-4ace318ade7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#In PySpark, the dropna() function, available on a DataFrame, is used to remove rows containing null or NaN (Not a Number) values.\n",
    "\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "# Clean df_customer\n",
    "df_customer_clean = (\n",
    "    df_customer\n",
    "    .dropna(subset=['customer_id'])\n",
    "    .dropDuplicates(['customer_id'])\n",
    "    .withColumn('first_name', lower(trim(col('first_name'))))\n",
    "    .withColumn('last_name', lower(trim(col('last_name'))))\n",
    "    .withColumn('email', trim(col('email')))\n",
    ")\n",
    "\n",
    "# Clean df_product\n",
    "df_product_clean = (\n",
    "    df_product\n",
    "    .dropna(subset=['product_id'])\n",
    "    .dropDuplicates(['product_id'])\n",
    "    .withColumn('product_name', lower(trim(col('product_name'))))\n",
    "    .withColumn('brand', trim(col('brand')))\n",
    ")\n",
    "\n",
    "# Clean df_date\n",
    "df_date_clean = (\n",
    "    df_date\n",
    "    .dropna(subset=['date_id'])\n",
    "    .dropDuplicates(['date_id'])\n",
    ")\n",
    "\n",
    "# Clean df_store\n",
    "df_store_clean = (\n",
    "    df_store\n",
    "    .dropna(subset=['store_id'])\n",
    "    .dropDuplicates(['store_id'])\n",
    "    .withColumn('store_name', trim(col('store_name')))\n",
    "    .withColumn('city', trim(col('city')))\n",
    "    .withColumn('region', trim(col('region')))\n",
    "    .withColumn('store_manager', trim(col('store_manager')))\n",
    ")\n",
    "\n",
    "# Clean df_fact\n",
    "df_fact_sales_clean = (\n",
    "    df_fact.dropna(subset=['sales_id'])\n",
    "           .dropDuplicates(['sales_id'])\n",
    ")\n",
    "\n",
    "df_fact_sales_clean.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd16a968-e283-4fb7-b249-9b0f663b0d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Normalize to Snowflake Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d3bd38-eb0d-4041-a10f-7ce74beaa3fb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "4": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753563656882}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 4
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, dense_rank\n",
    "# from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#monotonically_increasing_id() is a function available in Apache Spark's PySpark library (and other Spark APIs like Scala, Java, and R) that generates unique, monotonically increasing 64-bit integer IDs for rows within a DataFrame.\n",
    "\n",
    "# For Customer Table\n",
    "window_spec_loyalty = Window.orderBy('loyalty_status')\n",
    "\n",
    "df_loyalty_snowflake = df_customer_clean.select('loyalty_status').distinct().withColumn('loyaltyTier_id', dense_rank().over(window_spec_loyalty))\n",
    "\n",
    "df_loyalty_snowflake.display()\n",
    "\n",
    "\n",
    "df_customer_final_snowflake = df_customer_clean.join(df_loyalty_snowflake, on='loyalty_status', how='left').drop('loyalty_status')\n",
    "\n",
    "df_customer_final_snowflake.display()\n",
    "\n",
    "\n",
    "#For Product Table\n",
    "window_spec_brand = Window.orderBy('brand')\n",
    "df_brand_snowflake = df_product_clean.select('brand').distinct().withColumn('brand_id', dense_rank().over(window_spec_brand) )\n",
    "\n",
    "\n",
    "window_spec_category = Window.orderBy('category')\n",
    "df_category_snowflake = df_product_clean.select('category').distinct().withColumn('category_id', dense_rank().over(window_spec_category) )\n",
    "\n",
    "df_brand_snowflake.display()\n",
    "df_category_snowflake.display()\n",
    "df_product_final_snowflake = df_product_clean.join(df_brand_snowflake, on='brand', how='left').drop('brand').join(df_category_snowflake, on='category', how='left').drop('category')\n",
    "\n",
    "df_product_final_snowflake.display()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f875656b-70ff-4b0c-bd78-d56730baaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apply Complex transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff270e0-18b4-4f3f-b301-c9a40fd2d413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Enrich Product tier based on price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4582b7-25fb-4873-bd48-7936063a20ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_price = df_product_final_snowflake.withColumn('price-tier', when(col('price')>100, 'Premium').when(col('price')>50,'Standard').otherwise('Basic'))\n",
    "\n",
    "df_price.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d21a4-df51-4270-b270-d04ddc00f18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Filter future dates in facts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117bdde0-6adb-4af3-a0a6-a75d9b98ab9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "\n",
    "df_staging_filtered = df_staging.filter(col(\"last_updated\") == '2025-07-21T00:00:00.000+00:00')\n",
    "df_staging_filtered.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e389ae7-4df1-463e-9a9f-86d1a7ac174e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30463aa4-e916-4cf9-accc-0f7e010bfd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count\n",
    "df_loyalty_score = df_customer_final_snowflake.groupby('customer_id').agg(count('loyaltyTier_id').alias('loyalty_score'))\n",
    "df_loyalty_score.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7bd8f1-fa07-4de1-aa77-f386a35e4421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count\n",
    "df_fact_sales_clean.display()\n",
    "\n",
    "df_agg_fact = df_fact_sales_clean.groupBy('store_id').agg(count('product_id').alias('product_sold'), sum('quantity_sold').alias('total_quantity_sold'), sum('total_amount').alias('total_amount_sold'))\n",
    "\n",
    "df_agg_fact.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4056e4-23fd-4e1f-9a0f-51054f2d99ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write to Snowflake Staging tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a48aec9-6578-48d1-a5a2-7b74a5ced4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Snowflake Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f6bbb2-6c41-4cdf-8ff0-24503341b13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Snowflake Config\n",
    "sfOptions = {\n",
    "  \"sfURL\": \"https://fuskshz-jo76028.snowflakecomputing.com\",\n",
    "  \"sfUser\": \"adminuser\",\n",
    "  \"sfPassword\": \"Databasepass11!\",\n",
    "  \"sfDatabase\": \"sql_data_migration_snowflake\",\n",
    "  \"sfSchema\": \"data_migration_schema\",\n",
    "  \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "  \"sfRole\": \"accountadmin\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef2f4218-6c30-403f-aead-2b5dbac5acba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a4937b-faf7-4ecd-96fd-2e37887b615e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Since the tables are not writable to snowflake because it is using serverless cluster so it is not allowing the tables to write to snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5acc981-1dcf-4f15-b0a8-b43c4d50abcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Another option is push the transformaed and normalized data to ADLS and push to Snowflake through ADF pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8807690-9306-4f24-8233-dc82dcc5ef2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write the normalized and transformed data to ADLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65c1165-f2a5-4137-855c-5c10c84bad32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b588ff8-2a3c-4bea-8229-614d23b278ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "base_path = \"abfss://normalized-tables@blobstorageforcsvfiles.dfs.core.windows.net/\"\n",
    "\n",
    "\n",
    "\n",
    "def save_to_adls(df, table_name):\n",
    "    \n",
    "# Generate timestamp string\n",
    " timestamp = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "#  filename = f\"sales_{timestamp}.csv\"\n",
    "\n",
    "# Define ADLS path\n",
    " output_dir = f\"{base_path}tables\"\n",
    " final_path = f\"{base_path}snowflake-tables/{table_name}_{timestamp}.csv\"\n",
    "\n",
    "#Delete older files for the same table_name from target directory\n",
    " target_dir = f\"{base_path}snowflake-tables\"\n",
    " existing_files =dbutils.fs.ls(target_dir)\n",
    " for f in existing_files:\n",
    "    if f.name.startswith(f\"{table_name}_\") and f.name.endswith('.csv'):\n",
    "        dbutils.fs.rm(f.path, True)\n",
    "        print(f\"Deleted old file: {f.name}\")\n",
    "\n",
    "\n",
    "# Write to temp directory\n",
    " df.coalesce(2).write.option(\"header\", True).mode(\"overwrite\").csv(output_dir)\n",
    "\n",
    "# List files in temp directory to find the actual part file name\n",
    " files = dbutils.fs.ls(output_dir)\n",
    " part_file = [f.path for f in files if f.name.startswith(\"part-\")][0]\n",
    "\n",
    "# Copy to final path with parameterized filename\n",
    " dbutils.fs.cp(part_file, final_path)\n",
    "\n",
    "# Delete the temporary folder\n",
    " dbutils.fs.rm(output_dir, True)\n",
    " print(f\"✅ Saved new version: {table_name} → {final_path}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # df.coalesce(1).write.mode(\"overwrite\").format(\"csv\").options(header=\"true\", inferSchema=\"true\").save(path)\n",
    "# print(f\"✅ Saved: {table_name} → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637d99db-2ab5-4336-ad9e-735453318d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_to_adls(df_customer_final_snowflake,\"dbo_dim_customer\")\n",
    "save_to_adls(df_loyalty_snowflake,\"dbo_dim_loyalty\")\n",
    "save_to_adls(df_brand_snowflake,\"dbo_dim_brand\")\n",
    "save_to_adls(df_category_snowflake,\"dbo_dim_category\")\n",
    "save_to_adls(df_product_final_snowflake,\"dbo_dim_product\")\n",
    "save_to_adls(df_date_clean,\"dbo_dim_date\")\n",
    "save_to_adls(df_store_clean,\"dbo_dim_store\")\n",
    "save_to_adls(df_fact_sales_clean,\"dbo_dim_fact_sales\")\n",
    "save_to_adls(df_staging,\"dbo_dim_staging_fact_sales_incremental_loads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156f8293-e697-4571-a704-1565f4b2318e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19d6773-fbe0-4295-98a7-55ffe745487b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "base_path = \"abfss://normalized-tables@blobstorageforcsvfiles.dfs.core.windows.net/\"\n",
    "\n",
    "\n",
    "\n",
    "def save_transformed_data_to_adls(df, table_name):\n",
    "    \n",
    "# Generate timestamp string\n",
    " timestamp = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "#  filename = f\"sales_{timestamp}.csv\"\n",
    "\n",
    "# Define ADLS path\n",
    " output_dir = f\"{base_path}tables\"\n",
    " final_path = f\"{base_path}transformed-snowflake-tables/{table_name}_{timestamp}.csv\"\n",
    "\n",
    " #Delete older files for the same table_name from target directory\n",
    " target_dir = f\"{base_path}transformed-snowflake-tables\"\n",
    " existing_files =dbutils.fs.ls(target_dir)\n",
    " for f in existing_files:\n",
    "    if f.name.startswith(f\"{table_name}_\") and f.name.endswith('.csv'):\n",
    "        dbutils.fs.rm(f.path, True)\n",
    "        print(f\"Deleted old file: {f.name}\")\n",
    "\n",
    "# Write to temp directory\n",
    " df.coalesce(2).write.option(\"header\", True).mode(\"overwrite\").csv(output_dir)\n",
    "\n",
    "# List files in temp directory to find the actual part file name\n",
    " files = dbutils.fs.ls(output_dir)\n",
    "\n",
    " part_file = [f.path for f in files if f.name.startswith(\"part-\")][0]\n",
    " \n",
    "\n",
    "# Copy to final path with parameterized filename\n",
    " dbutils.fs.cp(part_file, final_path)\n",
    "\n",
    "# Delete the temporary folder\n",
    " dbutils.fs.rm(output_dir, True)\n",
    " print(f\"✅ Saved new version: {table_name} → {final_path}\")\n",
    "    \n",
    "    # df.coalesce(1).write.mode(\"overwrite\").format(\"csv\").options(header=\"true\", inferSchema=\"true\").save(path)\n",
    "# print(f\"✅ Saved: {table_name} → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950c8704-bc1d-4243-9ad2-efc8ac5288a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_transformed_data_to_adls(df_price, \"dbo_dim_price_tier\")\n",
    "save_transformed_data_to_adls(df_staging_filtered, \"dbo_dim_filtered_staging\")\n",
    "save_transformed_data_to_adls(df_loyalty_score, \"dbo_dim_loyalty_score\")\n",
    "save_transformed_data_to_adls(df_agg_fact, \"dbo_dim_agg_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c42184d-ede2-4f45-a802-5cd5c6be8a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Transformations 2025-07-26 12:31:24",
   "widgets": {
    "output_format": {
     "currentValue": "csv",
     "nuid": "c07d9788-2d8b-4538-b38a-76c829579b9d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "csv",
      "label": "Output Format",
      "name": "output_format",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "csv",
      "label": "Output Format",
      "name": "output_format",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "",
     "nuid": "df0ef7fa-64de-453d-a212-f4efbbdb833c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dim_customer",
      "label": "Table Name",
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dim_customer",
      "label": "Table Name",
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
